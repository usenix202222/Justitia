import os
import numpy
import ntpath
import json
import subprocess
import pandas as pd
from preprocess.read_labeled_sentence import read_data_from_google_csv

def get_path(folders):
    res = folders[0]
    for i in range(1, len(folders)):
        res = os.path.join(res, folders[i])
    return res

def get_data_set_info(dataframe):
    text = dataframe['text']
    length = [len(t.split()) for t in text]
    print("Length mean: ")
    print(numpy.mean(length))
    print("Max length")
    print(max(length))
    print("Min length")
    print(min(length))
    labels = [l for l in dataframe['labels']]
    count = dict()
    for t in labels:
        if t not in count:
            count[t] = 1
        else:
            count[t] = count[t] +1

    for t in count:
        print("Label {} has {} samples".format(t, count[t]))


def output_csv_to_txt_file(csv_path, target_file):
    sentences = read_data_from_google_csv(csv_path, {})
    with open(target_file, 'w+') as dst_file:
        for s in sentences:
            dst_file.write(s + os.linesep)


def output_sentence_with_given_ner_elements(file_path, ner_res_path, ner_tags, res_file):
    # with open(file_path, 'r') as src_path:
    #     sentences = src_path.readlines()
    print("Output sen")
    sentences = read_sentences_processed_by_stanford_ssplit(file_path)

    with open(ner_res_path, 'r') as src_path:
        json_ner_res = json.load(src_path)

    sentences_with_ner = json_ner_res['sentences']
    sentences_with_given_ner = set()
    for idx, sent in enumerate(sentences_with_ner):
        entitymentions = sent['entitymentions']
        for entity in entitymentions:
            ner_tag = entity['ner']
            if ner_tag in ner_tags:
                sentences_with_given_ner.add(idx)

    res = []
    for idx in sentences_with_given_ner:
        res.append(sentences[idx])

    with open(res_file, 'w+') as dst_file:
        for s in res:
            dst_file.write(s + os.linesep)


def process_input_with_nlp_ssplit(file_path, target_folder):
    '''

    :param file_path: text file from csv input
    :return: a txt file, each line is a sentence this is generated by stanford-nlp, it is used to locate sentence
    with special ner tag
    '''
    subprocess.run(["java", "-Xmx8g", "-cp", "/home/pxf109/stanford-corenlp-4.2.0/*",
                    "edu.stanford.nlp.pipeline.StanfordCoreNLP",
                    "-annotators", "tokenize,ssplit", "-file", file_path, "-outputFormat", "text", '-outputDirectory',
                    str(target_folder)])


def read_sentences_processed_by_stanford_ssplit(file_path):
    res = []
    with open(file_path, 'r') as src_path:
        lines = src_path.readlines()

    for i, cur_l in enumerate(lines):
        if cur_l.startswith("Sentence #"):
            next_line = i+1
            while next_line < len(lines) and lines[next_line] != os.linesep :
                res.append(lines[next_line].rstrip())
                next_line += 1
            # next_line = next_line.rstrip()
            # if next_line:
            #     res.append(next_line)

    return res


def convert_nlp_ssplit_output_to_csv(file_path, col_name, output_path):

    ssplit_res = read_sentences_processed_by_stanford_ssplit(file_path)

    res = dict()
    res[col_name] = ssplit_res

    data_frame = pd.DataFrame(data=res)

    data_frame.to_csv(output_path)


def output_nlp_ssplit_only_sentences(file_path, output_directory):

    ssplit_res = read_sentences_processed_by_stanford_ssplit(file_path)
    folder, tail = ntpath.split(file_path)
    target_folder =  os.path.join(output_directory, tail)

    with open(target_folder, 'w+') as dst_file:
        for s in ssplit_res:
            dst_file.write(s+os.linesep)


def convert_txt_to_csv(file_path, col_name, res_path):
    res = []
    with open(file_path, 'r') as src_file:
        for l in src_file.readlines():
            l = l.rstrip()
            if l:
                res.append(l)

    res_dict = dict()
    res_dict[col_name] = res

    data_frame = pd.DataFrame(data=res_dict)
    data_frame.to_csv(res_path)


def word_counts_for_sentences(sentence):
    tokens = sentence.split(" ")
    res = dict()
    for t in tokens:
        if t not in res.keys():
            res[t] = 0
        res[t] = res[t] + 1

    return res


def set_up_environment_variable():
    os.environ['CORENLP_HOME'] = "/home/pxf109/stanford-corenlp-4.2.0"


def check_and_modify_date(date):
    if date.find('day of') != -1:
        return date.replace('day of', '')
    return date


def current_string_is_float(string_digit):
    try:
        numbert = float(string_digit)
        return True
    except ValueError:
        return False


def convert_json_to_smartIR(ir):
    contracttype = ir['ContractCategory']
    sellername = ir['SellerName']
    buyername = ir['BuyerName']
    effectivetime = ''
    if len(ir['EffectiveTime']) > 0:
        effectivetime = ir['EffectiveTime'][0]
    closetime = "\"\""
    if len(ir['CloseTime']) > 0:
        closetime = ir['CloseTime'][0]
    expirytime = ''
    if len(ir['OutSideClosingDate']) > 0:
        expirytime = ir['OutSideClosingDate'][0]

    offlineconstraint = 'false'                         # need confirmed by zhenhua
    if ir['Transfers'] is True:
        offlineconstraint = 'hash'

    paymentdeliverconstraint = 'false'
    if ir['Payments'][0]['Transfer'] is True:
        paymentdeliverconstraint = 'true'

    price = ir['Payments'][0]['PurchasePrice']

    #Termination

    terminatedeliver = 'false'
    if ir['Terminations']['TransferTermination'] == True:
        terminatedeliver = 'true'

    othertermindate = 'false'
    if ir['Terminations']['OtherTermination'] == True:
        othertermindate = 'true'

    smartIR = f'''ContractCategory: {contracttype};

Entity: {{
    SellerNames: {sellername};
    BuyerNames: {buyername};
}};

EffectiveTime: {effectivetime};
CloseTime: {closetime};
ExpiryTime: {expirytime};

OfflineDelivery: {{
    DeliveryConstraint: {offlineconstraint};
}};

OnlineStateTransfer: [{{
    TimeConstraint: {{
        operator: <=,
        leftOprand: now,
        rightOprand: CloseTime
    }};
    DeliveryConstraint: {paymentdeliverconstraint};
    (TimeContraint && DeliveryConstraint) -> Payment {{
         From: {buyername};
         To: {sellername};
         Price: {{
             Amount: {price},
             Unit: USD
         }}
    }};
}}];

Termination: {{
    TimeConstraint: {{
        operator: >=,
        leftOprand: now,
        rightOprand: ExpiryTime
    }};
    DeliveryConstraint: {terminatedeliver};
    OtherConstraint: {othertermindate};
    (TimeConstraint || DeliveryConstraint || OtherConstraint);
}};'''
    return smartIR

